{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyO2xqzbnr4OUwTFMJFwYJ76",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rgobinat/NLP-Projects/blob/master/Shakespeare_Text_Generation_with_an_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wrrinpuyQ0I-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#Data Preprocessing\n",
        "#Load File\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "#define Vocab\n",
        "text = open(path_to_file, 'rb').read().decode(encoding = 'utf-8')\n",
        "vocab = sorted(set(text))\n",
        "\n",
        "#Char to ID & Id to Char using StringLookup Model\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary = list(vocab), mask_token = None)\n",
        "chars_from_ids = tf.keras.layers.StringLookup(vocabulary = ids_from_chars.get_vocabulary(),invert=True, mask_token=None )\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "\n",
        "#Prepare Dataset for Training\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "\n",
        "#Create a TensorFlow DataSet\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "seq_length = 100\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "#Split sequences into Xs and Ys\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "#Dataset to input to Model\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DO NOT RUN THIS\n",
        "#Same Code as above **** Ignore\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary = list(vocab), mask_token = None)\n",
        "chars_from_ids = tf.keras.layers.StringLookup(vocabulary = ids_from_chars.get_vocabulary(), invert = True, mask_token = None)\n",
        "def texts_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis = -1)\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "seq_length = 100\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "def split_sequences(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text\n",
        "dataset = sequences.map(split_sequences)\n",
        "BATCH_SIZE=64\n",
        "BUFFER_SIZE=10000\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "x2FR3yxdb0hq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os as os\n",
        "\n",
        "#Enable runtime to GPU for faster model training\n",
        "\n",
        "#Deine, Compile and Fit the Model\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "\n",
        "\n",
        "#Try the Model - Untrained\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())\n",
        "#TRY\n",
        "\n",
        "\n",
        "#Train the Model\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)\n",
        "tf.exp(example_batch_mean_loss).numpy()\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "model.summary()\n",
        "\n",
        "#Configure Checkpoints\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "#Execute Training\n",
        "EPOCHS = 20\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFFmSlQKb2Pl",
        "outputId": "d9e7ded6-e135-4884-e619-31904a2fe4e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
            "Input:\n",
            " b\" seeth not this palpable device?\\nYet who's so blind, but says he sees it not?\\nBad is the world; and \"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"W\\nNx:gZbZ:FgJ[UNK]lwB:EPfr,TDF;fSsn;QWNF;RhTGHsiwJNExuY'ztiFOY;VTmJ3'YSlkagxuEBO BXZF:XWCKrlxzLzyccnkXrq\"\n",
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.189696, shape=(), dtype=float32)\n",
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "172/172 [==============================] - 7s 17ms/step - loss: 2.7167\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 1.9832\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 1.7043\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 1.5475\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 1.4503\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 1.3820\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 3s 11ms/step - loss: 1.3289\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 1.2851\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 1.2437\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 3s 11ms/step - loss: 1.2045\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 1.1631\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 1.1229\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 1.0790\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 1.0333\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 0.9857\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 0.9340\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 0.8821\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 0.8290\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 0.7786\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 3s 10ms/step - loss: 0.7280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Geenrate Text\n",
        "import time\n",
        "\n",
        "seed_text = 'Shylock:'\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([seed_text])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DM-03P_Eh7Md",
        "outputId": "61f3696e-1b59-42c7-b99a-4649833c5d5f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shylock: there's a my bloody, treacherous,\n",
            "More than the world and should be his done. Come, officer:\n",
            "The more he is faked for the new-deliver'd;\n",
            "He shall appear it in yonder enough:'\n",
            "bear ye good to him! Come on, come hither;\n",
            "But as you will command your famous loan,\n",
            "As rice prepare for all the which your pidges put by\n",
            "likeness of the miracles of inevite:\n",
            "But for the loving presence may have we? I will perform'd\n",
            "My wretchedness but thirst will be my drops?\n",
            "\n",
            "TYBROSS OF YORK:\n",
            "Grave heart?\n",
            "\n",
            "CORIOLANUS:\n",
            "The thirf, your sheep-speed, sir.\n",
            "\n",
            "BRUTUS:\n",
            "'Tis dead;\n",
            "By thieves I could with those that I love that he hath,\n",
            "By 'twere expected in his holy house.\n",
            "This lie, how canst thou made me spile away?\n",
            "\n",
            "RATCLIFF:\n",
            "Where is he?\n",
            "\n",
            "GRUMIO:\n",
            "Renowned all to the end.\n",
            "\n",
            "CLAUDIO:\n",
            "Then show't thyself will off you to the trial; and besee\n",
            "The tread of traitor spices of histingly there,\n",
            "By draw any meredest father's life to life\n",
            "What you cannot had ere it proclaimed aid.\n",
            "Alone his sister! kiss me ere itward son,\n",
            "And leav \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.5012614727020264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hNUratnSnI5y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}